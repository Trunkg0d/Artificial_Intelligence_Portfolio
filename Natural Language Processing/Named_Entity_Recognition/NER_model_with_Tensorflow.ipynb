{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b0b12d-44d3-474c-bd40-c49139596b63",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name=\"0\"></a>\n",
    "## Introduction\n",
    "\n",
    "We first start by defining named entity recognition (NER). NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. \n",
    "\n",
    "For example:\n",
    "\n",
    "<img src = 'images/ner.png' width=\"width\" height=\"height\" style=\"width:600px;height:150px;\"/>\n",
    "\n",
    "Is labeled as follows: \n",
    "\n",
    "- French: geopolitical entity\n",
    "- Morocco: geographic entity \n",
    "- Christmas: time indicator\n",
    "\n",
    "Everything else that is labeled with an `O` is not considered to be a named entity. In this assignment, we will train a named entity recognition system that could be trained in a few seconds (on a GPU) and will get around 75% accuracy. Then, we will load in the exact version of your model, which was trained for a longer period of time. We could then evaluate the trained version of our model to get 96% accuracy! Finally, we will be able to test our named entity recognition system with our own sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0d50ab-7b9f-4efd-a358-321f0c020dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 12:33:44.397673: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-09 12:33:45.175601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a3bf7-db2a-4be2-8d16-956e1160540f",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Exploring the Data\n",
    "\n",
    "We will be using a dataset from Kaggle, which we will preprocess. The original data consists of four columns: the sentence number, the word, the part of speech of the word, and the tags.  A few tags we might expect to see are: \n",
    "\n",
    "* geo: geographical entity\n",
    "* org: organization\n",
    "* per: person \n",
    "* gpe: geopolitical entity\n",
    "* tim: time indicator\n",
    "* art: artifact\n",
    "* eve: event\n",
    "* nat: natural phenomenon\n",
    "* O: filler word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63c1286-0e95-48a8-97f5-dcb6d80ffb7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vocab(vocab_path, tags_path):\n",
    "    vocab = {}\n",
    "    with open(vocab_path, encoding=\"utf-8\") as f:\n",
    "        for i, l in enumerate(f.read().splitlines()):\n",
    "            vocab[l] = i  # to avoid the 0\n",
    "        # loading tags (we require this to map tags to their indices)\n",
    "    vocab['<PAD>'] = len(vocab) # 35180\n",
    "    tag_map = {}\n",
    "    with open(tags_path) as f:\n",
    "        for i, t in enumerate(f.read().splitlines()):\n",
    "            tag_map[t] = i \n",
    "    \n",
    "    return vocab, tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8324ee-9a76-4d5e-9046-59659efb3ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_params(vocab, tag_map, sentences_file, labels_file):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(sentences_file, encoding=\"utf-8\") as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            # replace each token by its index if it is in vocab\n",
    "            # else use index of UNK_WORD\n",
    "            s = [vocab[token] if token in vocab \n",
    "                 else vocab['UNK']\n",
    "                 for token in sentence.split(' ')]\n",
    "            sentences.append(s)\n",
    "\n",
    "    with open(labels_file) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            # replace each label by its index\n",
    "            l = [tag_map[label] for label in sentence.split(' ')] # I added plus 1 here\n",
    "            labels.append(l) \n",
    "    return sentences, labels, len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6916bc6-ea38-453e-980b-b4817da25ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab, tag_map = get_vocab('data/large/words.txt', 'data/large/tags.txt')\n",
    "t_sentences, t_labels, t_size = get_params(vocab, tag_map, 'data/large/train/sentences.txt', 'data/large/train/labels.txt')\n",
    "v_sentences, v_labels, v_size = get_params(vocab, tag_map, 'data/large/val/sentences.txt', 'data/large/val/labels.txt')\n",
    "test_sentences, test_labels, test_size = get_params(vocab, tag_map, 'data/large/test/sentences.txt', 'data/large/test/labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fca3e1e-6c3f-4106-a9a3-b264d082fab5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab[\"the\"]: 9\n",
      "padded token: 35180\n"
     ]
    }
   ],
   "source": [
    "# vocab translates from a word to a unique number\n",
    "print('vocab[\"the\"]:', vocab[\"the\"])\n",
    "# Pad token\n",
    "print('padded token:', vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b28e89-6228-451f-8dd6-eed048a28055",
   "metadata": {},
   "source": [
    "`vocab` is a dictionary that translates a word string to a unique number. Given a sentence, we can represent it as an array of numbers translating with this dictionary. The dictionary contains a `<PAD>` token. \n",
    "\n",
    "When training an LSTM using batches, all our input sentences must be the same size. To accomplish this, we set the length of our sentences to a certain number and add the generic `<PAD>` token to fill all the empty spaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfebe893-bffc-432c-87db-80f9e38eef1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `tag_map` is a dictionary that maps the tags that we could have to numbers. Run the cell below to see the possible classes we will be predicting. The prepositions in the tags mean:\n",
    "* I: Token is inside an entity.\n",
    "* B: Token begins an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd35ffc8-3a4d-4ea2-a683-41c365ca253b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-gpe': 2,\n",
       " 'B-per': 3,\n",
       " 'I-geo': 4,\n",
       " 'B-org': 5,\n",
       " 'I-org': 6,\n",
       " 'B-tim': 7,\n",
       " 'B-art': 8,\n",
       " 'I-art': 9,\n",
       " 'I-per': 10,\n",
       " 'I-gpe': 11,\n",
       " 'I-tim': 12,\n",
       " 'B-nat': 13,\n",
       " 'B-eve': 14,\n",
       " 'I-eve': 15,\n",
       " 'I-nat': 16}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba9391-3973-41bc-84b7-54e8190172bf",
   "metadata": {},
   "source": [
    "If we had the sentence \n",
    "\n",
    "**\"Sharon flew to Miami on Friday\"**\n",
    "\n",
    "The tags would look like:\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "where we would have three tokens beginning with B-, since there are no multi-token entities in the sequence. But if we added Sharon's last name to the sentence:\n",
    "\n",
    "**\"Sharon Floyd flew to Miami on Friday\"**\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "Floyd  I-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "our tags would change to show first \"Sharon\" as B-per, and \"Floyd\" as I-per, where I- indicates an inner token in a multi-token sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf722f9-2f79-4421-9037-a87c86934cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of outputs is tag_map 17\n",
      "Num of vocabulary words: 35181\n",
      "The training size is 33570\n",
      "The validation size is 7194\n",
      "An example of the first sentence is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]\n",
      "An example of its corresponding label is [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Exploring information about the data\n",
    "print('The number of outputs is tag_map', len(tag_map))\n",
    "# The number of vocabulary tokens (including <PAD>)\n",
    "g_vocab_size = len(vocab)\n",
    "print(f\"Num of vocabulary words: {g_vocab_size}\")\n",
    "print('The training size is', t_size)\n",
    "print('The validation size is', v_size)\n",
    "print('An example of the first sentence is', t_sentences[0])\n",
    "print('An example of its corresponding label is', t_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e46d6a-2b51-4a76-b694-847d7641367c",
   "metadata": {},
   "source": [
    "### 1.2 - Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa583d6-8b4b-4132-b530-ec679cd9a074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stack_tensor(lines, labels, pad = vocab[\"<PAD>\"]):\n",
    "    max_len = 0\n",
    "    for line in lines:\n",
    "        if len(line) > max_len:\n",
    "            max_len = len(line)\n",
    "    \n",
    "    stack_tensor_lines = []\n",
    "    for line in lines:\n",
    "        line += [pad] * (max_len - len(line))\n",
    "        stack_tensor_lines.append(line)\n",
    "        \n",
    "    stack_tensor_labels = []\n",
    "    for label_lines in labels:\n",
    "        label_lines += [0] * (max_len - len(label_lines))\n",
    "        stack_tensor_labels.append(label_lines)\n",
    "    \n",
    "    return (np.array(stack_tensor_lines), np.array(stack_tensor_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6870a028-a480-43e9-b36c-7b4c3b181b81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     1,     2, ..., 35180, 35180, 35180],\n",
       "       [   22,     1,    23, ..., 35180, 35180, 35180],\n",
       "       [   42,     4,    18, ..., 35180, 35180, 35180],\n",
       "       ...,\n",
       "       [29838, 29839,  6586, ..., 35180, 35180, 35180],\n",
       "       [ 1001, 29840, 29841, ..., 35180, 35180, 35180],\n",
       "       [ 3175,   502,  2543, ..., 35180, 35180, 35180]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines, train_labels = stack_tensor(t_sentences, t_labels)\n",
    "eval_lines, eval_labels = stack_tensor(v_sentences, v_labels)\n",
    "test_lines, test_labels = stack_tensor(test_sentences, test_labels)\n",
    "train_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92dcaa2e-b5ef-425c-9bc5-066ab97fe21b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 12:33:46.812164: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 12:33:46.812295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 12:33:46.883632: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "train_lines = tf.data.Dataset.from_tensor_slices(train_lines)\n",
    "eval_lines = tf.data.Dataset.from_tensor_slices(eval_lines)\n",
    "test_lines = tf.data.Dataset.from_tensor_slices(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e05801a-4b70-4cbf-ae60-84e9136414a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(104,), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfbaa14d-a5dd-4771-95b0-54f8ba4131a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_targets = tf.data.Dataset.from_tensor_slices(train_labels)\n",
    "eval_targets = tf.data.Dataset.from_tensor_slices(eval_labels)\n",
    "test_targets = tf.data.Dataset.from_tensor_slices(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "777d6f47-98ed-4d69-8f6a-f99f5b91abe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(104,), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c0b59a2-87b7-4670-9670-e647498e7a17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.zip((train_lines, train_targets))\n",
    "eval_dataset = tf.data.Dataset.zip((eval_lines, eval_targets))\n",
    "test_dataset = tf.data.Dataset.zip((test_lines, test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6a2ef8b-0376-49f1-8c61-c07d2ecd5a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder = True)\n",
    "eval_dataset = eval_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder = True)\n",
    "test_dataset = test_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d8ae7-7017-4c66-ac9e-457651e1d7d0",
   "metadata": {},
   "source": [
    "## 2 - Building the Model\n",
    "\n",
    "We will now implement the model that will be able to determining the tags of sentences like the following:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "<img src = 'images/ner1.png' width=\"width\" height=\"height\" style=\"width:500px;height:150px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The model architecture will be as follows: \n",
    "\n",
    "<img src = 'images/ner2.png' width=\"width\" height=\"height\" style=\"width:600px;height:250px;\"/>\n",
    "\n",
    "\n",
    "Concretely, our inputs will be sentences represented as tensors that are fed to a model with:\n",
    "\n",
    "* An Embedding layer,\n",
    "* A LSTM layer\n",
    "* A Dense layer\n",
    "* A log softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "079555d1-b090-4bf0-81e7-7f19d1828190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_size = 50\n",
    "\n",
    "def build_model(tags, vocab_size = vocab_size, embedding_size = embedding_size):\n",
    "    '''\n",
    "    Input:\n",
    "        tags - dictionary that maps the tags to the numbers\n",
    "        vocab_size - integer containing the size of the vocabulary\n",
    "        embedding_size - integer describing the embedding size\n",
    "    Output:\n",
    "        model - a sequential model\n",
    "    '''\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_size),\n",
    "        tf.keras.layers.Masking(mask_value=vocab[\"<PAD>\"]),\n",
    "        tf.keras.layers.LSTM(units=embedding_size,\n",
    "                             return_sequences=True,\n",
    "                             recurrent_initializer=\"glorot_uniform\"),\n",
    "        tf.keras.layers.Dense(len(tags), activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "246410bc-9704-47b6-906d-36f66f573495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          1759050   \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 50)          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 50)          20200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 17)          867       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1780117 (6.79 MB)\n",
      "Trainable params: 1780117 (6.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(tag_map, vocab_size, embedding_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0857c87-b2cb-4529-a460-de257561f095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07f2debe-2e77-4664-b4e1-359c05c3ba8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034fa95a-19c7-4c40-88ce-11c70e5c6d74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3 - Train the Model \n",
    "\n",
    "This section will train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d119de2d-b631-440c-a08a-351bbf8cf7df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-iec_cybercode/.local/lib/python3.9/site-packages/keras/src/backend.py:5714: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524/524 [==============================] - 17s 30ms/step - loss: 0.2569 - accuracy: 0.9652 - val_loss: 0.1354 - val_accuracy: 0.9618\n",
      "Epoch 2/15\n",
      "524/524 [==============================] - 15s 29ms/step - loss: 0.0783 - accuracy: 0.9774 - val_loss: 0.0941 - val_accuracy: 0.9737\n",
      "Epoch 3/15\n",
      "524/524 [==============================] - 15s 29ms/step - loss: 0.0512 - accuracy: 0.9872 - val_loss: 0.0618 - val_accuracy: 0.9851\n",
      "Epoch 4/15\n",
      "524/524 [==============================] - 15s 29ms/step - loss: 0.0337 - accuracy: 0.9915 - val_loss: 0.0489 - val_accuracy: 0.9868\n",
      "Epoch 5/15\n",
      "524/524 [==============================] - 15s 29ms/step - loss: 0.0264 - accuracy: 0.9928 - val_loss: 0.0452 - val_accuracy: 0.9872\n",
      "Epoch 6/15\n",
      "524/524 [==============================] - 15s 29ms/step - loss: 0.0226 - accuracy: 0.9936 - val_loss: 0.0438 - val_accuracy: 0.9873\n",
      "Epoch 7/15\n",
      "524/524 [==============================] - 15s 29ms/step - loss: 0.0199 - accuracy: 0.9940 - val_loss: 0.0435 - val_accuracy: 0.9875\n",
      "Epoch 8/15\n",
      "524/524 [==============================] - 15s 29ms/step - loss: 0.0180 - accuracy: 0.9945 - val_loss: 0.0437 - val_accuracy: 0.9872\n",
      "Epoch 9/15\n",
      "524/524 [==============================] - 15s 29ms/step - loss: 0.0166 - accuracy: 0.9948 - val_loss: 0.0443 - val_accuracy: 0.9872\n",
      "Epoch 10/15\n",
      "524/524 [==============================] - 15s 29ms/step - loss: 0.0155 - accuracy: 0.9951 - val_loss: 0.0454 - val_accuracy: 0.9869\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=eval_dataset,\n",
    "                    epochs=EPOCHS, \n",
    "                    callbacks=[checkpoint_callback, tf.keras.callbacks.EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ddc79-9643-4027-bade-d5fc2c5478e1",
   "metadata": {},
   "source": [
    "## 4 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c547b0e2-a84a-4fc9-80a4-37c2264b49b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 1s 6ms/step - loss: 0.0454 - accuracy: 0.9869\n",
      "Loss on test data:  0.04541099816560745\n",
      "Accuracy on test data:  0.9869377613067627\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(eval_dataset, batch_size=1)\n",
    "print(\"Loss on test data: \", loss)\n",
    "print(\"Accuracy on test data: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7a036-33e2-498a-ba6b-7636f2e1b728",
   "metadata": {},
   "source": [
    "## 5 - Testing with our own Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "921ec55b-e844-446f-96b1-996ee2c7892e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the function we will be using to test our own sentence.\n",
    "def predict(sentence, model, vocab, tag_map):\n",
    "    s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "    batch_data = np.ones((1, len(s)))\n",
    "    batch_data[0][:] = s\n",
    "    sentence = np.array(batch_data).astype(int)\n",
    "    output = model(sentence)\n",
    "    outputs = np.argmax(output, axis=2)\n",
    "    labels = list(tag_map.keys())\n",
    "    pred = []\n",
    "    for i in range(len(outputs[0])):\n",
    "        idx = outputs[0][i] \n",
    "        pred_label = labels[idx]\n",
    "        pred.append(pred_label)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "202a4439-25a8-4c4d-9268-5ce42968d172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter B-per\n",
      "Navarro, I-per\n",
      "White B-org\n",
      "House I-org\n",
      "Sunday B-tim\n",
      "morning I-tim\n",
      "White B-org\n",
      "House I-org\n",
      "coronavirus B-org\n",
      "fall, I-geo\n"
     ]
    }
   ],
   "source": [
    "# Try the output for the introduction example\n",
    "#sentence = \"Many French citizens are goin to visit Morocco for summer\"\n",
    "#sentence = \"Sharon Floyd flew to Miami last Friday\"\n",
    "\n",
    "# New york times news:\n",
    "sentence = \"Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldn’t necessarily come\"\n",
    "s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "predictions = predict(sentence, model, vocab, tag_map)\n",
    "for x,y in zip(sentence.split(' '), predictions):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be241262-8a99-4947-be8a-da8c733f470a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob B-per\n",
      "Robinson, I-per\n",
      "I'm I-per\n",
      "Viet B-geo\n",
      "Nam I-per\n",
      "University. I-per\n",
      "Thursday B-tim\n",
      "I I-tim\n",
      "University B-org\n"
     ]
    }
   ],
   "source": [
    "sentence = \"My name is Bob Robinson, I'm from Viet Nam and now I studying at Standford University. In Thursday I will have the first class at this University\"\n",
    "s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "predictions = predict(sentence, model, vocab, tag_map)\n",
    "for x,y in zip(sentence.split(' '), predictions):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92b73b4c-08fd-4b32-9e70-411b35c10e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuesday B-tim\n",
      "Manhattan B-geo\n",
      "New I-geo\n",
      "York I-geo\n",
      "China B-geo\n",
      "Li B-per\n",
      "Fang I-per\n",
      "Wei I-per\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Tuesday the Manhattan New York city prosecutor unsealed a multi count indictment against China based limmt economic and trade company and Li Fang Wei one of the firm 's managers\"\n",
    "s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "predictions = predict(sentence, model, vocab, tag_map)\n",
    "for x,y in zip(sentence.split(' '), predictions):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef33e086-a774-479f-8dfb-fdd7aaf93cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharon B-per\n",
      "Floyd I-per\n",
      "Miami B-geo\n",
      "Friday B-tim\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Sharon Floyd flew to Miami on Friday\"\n",
    "s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "predictions = predict(sentence, model, vocab, tag_map)\n",
    "for x,y in zip(sentence.split(' '), predictions):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefa286-672f-4744-94c3-0ab003c0be94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_kernel",
   "language": "python",
   "name": "test_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
