{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300dc838",
   "metadata": {},
   "source": [
    "# Deep N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754f5fc5",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Overview\n",
    "\n",
    "Our task will be to predict the next set of characters using the previous characters. \n",
    "- Although this task sounds simple, it is pretty useful.\n",
    "- We will start by converting a line of text into a tensor\n",
    "- Then we will create a generator to feed data into the model\n",
    "- We will train a neural network in order to predict the new set of characters of defined length. \n",
    "- We will use embeddings for each character and feed them as inputs to our model. \n",
    "    - Many natural language tasks rely on using embeddings for predictions. \n",
    "- Our model will convert each character to its embedding, run the embeddings through a Gated Recurrent Unit `GRU`, and run it through a linear layer to predict the next set of characters.\n",
    "\n",
    "<img src = \"images/model.png\" style=\"width:600px;height:150px;\"/>\n",
    "\n",
    "The figure above gives us a summary of what we are about to implement. \n",
    "- We will get the embeddings;\n",
    "- Stack the embeddings on top of each other;\n",
    "- Run them through two layers with a relu activation in the middle;\n",
    "- Finally, we will compute the softmax. \n",
    "\n",
    "To predict the next character:\n",
    "- Use the softmax output and identify the word with the highest probability.\n",
    "- The word with the highest probability is the prediction for the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64bcf13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Trung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.data.path.append('.')\n",
    "\n",
    "# set random seed\n",
    "rnd.seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac3d4a5",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Importing the Data\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - Loading in the Data\n",
    "\n",
    "<img src = \"images/shakespeare.png\" style=\"width:250px;height:250px;\"/>\n",
    "\n",
    "Now import the dataset and do some processing. \n",
    "- The dataset has one sentence per line.\n",
    "- We will be doing word generation, so we have to process each sentence by converting each **word** to a number. \n",
    "- Store each line in a list.\n",
    "- Create a data generator that takes in the `batch_size` and the `max_length`. \n",
    "    - The `max_length` corresponds to the maximum length of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05b95d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'data/'\n",
    "filename = 'shakespeare_data.txt'\n",
    "lines = [] # storing all the lines in a variable. \n",
    "\n",
    "counter = 0\n",
    "\n",
    "with open(os.path.join(dirname, filename)) as files:\n",
    "    for line in files:        \n",
    "        # remove leading and trailing whitespace\n",
    "        pure_line = line.strip()\n",
    "\n",
    "        # if pure_line is not the empty string,\n",
    "        if pure_line:\n",
    "            # append it to the list\n",
    "            lines.append(pure_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e6854fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 125097\n",
      "Sample line at position 0 A LOVER'S COMPLAINT\n",
      "Sample line at position 999 With this night's revels and expire the term\n"
     ]
    }
   ],
   "source": [
    "n_lines = len(lines)\n",
    "print(f\"Number of lines: {n_lines}\")\n",
    "print(f\"Sample line at position 0 {lines[0]}\")\n",
    "print(f\"Sample line at position 999 {lines[999]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c978b04",
   "metadata": {},
   "source": [
    "Notice that the letters are both uppercase and lowercase.  In order to reduce the complexity of the task, we will convert all characters to lowercase.  This way, the model only needs to predict the likelihood that a letter is 'a' and not decide between uppercase 'A' and lowercase 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "faae537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 125097\n",
      "Sample line at position 0 a lover's complaint\n",
      "Sample line at position 999 with this night's revels and expire the term\n"
     ]
    }
   ],
   "source": [
    "# go through each line\n",
    "for i, line in enumerate(lines):\n",
    "    # convert to all lowercase\n",
    "    lines[i] = line.lower()\n",
    "\n",
    "print(f\"Number of lines: {n_lines}\")\n",
    "print(f\"Sample line at position 0 {lines[0]}\")\n",
    "print(f\"Sample line at position 999 {lines[999]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c159cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines for training: 123597\n",
      "Number of lines for validation: 1000\n",
      "Number of lines for testing: 500\n"
     ]
    }
   ],
   "source": [
    "test_lines = lines[-500:]\n",
    "eval_lines = lines[-1500:-500] # Create a holdout validation set\n",
    "lines = lines[:-1500] # Leave the rest for training\n",
    "\n",
    "print(f\"Number of lines for training: {len(lines)}\")\n",
    "print(f\"Number of lines for validation: {len(eval_lines)}\")\n",
    "print(f\"Number of lines for testing: {len(test_lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e85d8",
   "metadata": {},
   "source": [
    "### Process tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48ba8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "    \n",
    "    '''\n",
    "    tweet = tweet.lower()\n",
    "    return nltk.word_tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c69a1f",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Convert a Line to Tensor\n",
    "\n",
    "Now that we have our list of lines, we will convert each word in that list to a number. First we create a vocabulary for words in lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7525766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(lines, EOS_int=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        lines(str): Lines of text\n",
    "        EOS_int (int, optional): End-of-sentence integer. Defaults to 1.\n",
    "    Returns:\n",
    "        vocab (dict): Dictionary map each word with their index.\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    vocab[\"EOS_int\"] = 1\n",
    "    for line in lines:\n",
    "        processed_line = process_tweet(line)\n",
    "        for word in processed_line:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab) + 1\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e2cd16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EOS_int': 1,\n",
       " 'a': 2,\n",
       " 'lover': 3,\n",
       " \"'s\": 4,\n",
       " 'complaint': 5,\n",
       " 'from': 6,\n",
       " 'off': 7,\n",
       " 'hill': 8,\n",
       " 'whose': 9,\n",
       " 'concave': 10,\n",
       " 'womb': 11,\n",
       " 'reworded': 12,\n",
       " 'plaintful': 13,\n",
       " 'story': 14,\n",
       " 'sistering': 15,\n",
       " 'vale': 16,\n",
       " ',': 17,\n",
       " 'my': 18,\n",
       " 'spirits': 19,\n",
       " 'to': 20,\n",
       " 'attend': 21,\n",
       " 'this': 22,\n",
       " 'double': 23,\n",
       " 'voice': 24,\n",
       " 'accorded': 25,\n",
       " 'and': 26,\n",
       " 'down': 27,\n",
       " 'i': 28,\n",
       " 'laid': 29,\n",
       " 'list': 30,\n",
       " 'the': 31,\n",
       " 'sad-tuned': 32,\n",
       " 'tale': 33,\n",
       " ';': 34,\n",
       " 'ere': 35,\n",
       " 'long': 36,\n",
       " 'espied': 37,\n",
       " 'fickle': 38,\n",
       " 'maid': 39,\n",
       " 'full': 40,\n",
       " 'pale': 41,\n",
       " 'tearing': 42,\n",
       " 'of': 43,\n",
       " 'papers': 44,\n",
       " 'breaking': 45,\n",
       " 'rings': 46,\n",
       " 'a-twain': 47,\n",
       " 'storming': 48,\n",
       " 'her': 49,\n",
       " 'world': 50,\n",
       " 'with': 51,\n",
       " 'sorrow': 52,\n",
       " 'wind': 53,\n",
       " 'rain': 54,\n",
       " '.': 55,\n",
       " 'upon': 56,\n",
       " 'head': 57,\n",
       " 'platted': 58,\n",
       " 'hive': 59,\n",
       " 'straw': 60,\n",
       " 'which': 61,\n",
       " 'fortified': 62,\n",
       " 'visage': 63,\n",
       " 'sun': 64,\n",
       " 'whereon': 65,\n",
       " 'thought': 66,\n",
       " 'might': 67,\n",
       " 'think': 68,\n",
       " 'sometime': 69,\n",
       " 'it': 70,\n",
       " 'saw': 71,\n",
       " 'carcass': 72,\n",
       " 'beauty': 73,\n",
       " 'spent': 74,\n",
       " 'done': 75,\n",
       " ':': 76,\n",
       " 'time': 77,\n",
       " 'had': 78,\n",
       " 'not': 79,\n",
       " 'scythed': 80,\n",
       " 'all': 81,\n",
       " 'that': 82,\n",
       " 'youth': 83,\n",
       " 'begun': 84,\n",
       " 'nor': 85,\n",
       " 'quit': 86,\n",
       " 'but': 87,\n",
       " 'spite': 88,\n",
       " 'heaven': 89,\n",
       " 'fell': 90,\n",
       " 'rage': 91,\n",
       " 'some': 92,\n",
       " 'peep': 93,\n",
       " \"'d\": 94,\n",
       " 'through': 95,\n",
       " 'lattice': 96,\n",
       " 'sear': 97,\n",
       " 'age': 98,\n",
       " 'oft': 99,\n",
       " 'did': 100,\n",
       " 'she': 101,\n",
       " 'heave': 102,\n",
       " 'napkin': 103,\n",
       " 'eyne': 104,\n",
       " 'on': 105,\n",
       " 'conceited': 106,\n",
       " 'characters': 107,\n",
       " 'laundering': 108,\n",
       " 'silken': 109,\n",
       " 'figures': 110,\n",
       " 'in': 111,\n",
       " 'brine': 112,\n",
       " 'season': 113,\n",
       " 'woe': 114,\n",
       " 'pelleted': 115,\n",
       " 'tears': 116,\n",
       " 'often': 117,\n",
       " 'reading': 118,\n",
       " 'what': 119,\n",
       " 'contents': 120,\n",
       " 'bears': 121,\n",
       " 'as': 122,\n",
       " 'shrieking': 123,\n",
       " 'undistinguish': 124,\n",
       " 'clamours': 125,\n",
       " 'size': 126,\n",
       " 'both': 127,\n",
       " 'high': 128,\n",
       " 'low': 129,\n",
       " 'sometimes': 130,\n",
       " 'levell': 131,\n",
       " 'eyes': 132,\n",
       " 'their': 133,\n",
       " 'carriage': 134,\n",
       " 'ride': 135,\n",
       " 'they': 136,\n",
       " 'battery': 137,\n",
       " 'spheres': 138,\n",
       " 'intend': 139,\n",
       " 'diverted': 140,\n",
       " 'poor': 141,\n",
       " 'balls': 142,\n",
       " 'are': 143,\n",
       " 'tied': 144,\n",
       " 'orbed': 145,\n",
       " 'earth': 146,\n",
       " 'do': 147,\n",
       " 'extend': 148,\n",
       " 'view': 149,\n",
       " 'right': 150,\n",
       " 'anon': 151,\n",
       " 'gazes': 152,\n",
       " 'lend': 153,\n",
       " 'every': 154,\n",
       " 'place': 155,\n",
       " 'at': 156,\n",
       " 'once': 157,\n",
       " 'nowhere': 158,\n",
       " 'fix': 159,\n",
       " 'mind': 160,\n",
       " 'sight': 161,\n",
       " 'distractedly': 162,\n",
       " 'commix': 163,\n",
       " 'hair': 164,\n",
       " 'loose': 165,\n",
       " 'formal': 166,\n",
       " 'plat': 167,\n",
       " 'proclaim': 168,\n",
       " 'careless': 169,\n",
       " 'hand': 170,\n",
       " 'pride': 171,\n",
       " 'for': 172,\n",
       " 'untuck': 173,\n",
       " 'descended': 174,\n",
       " 'sheaved': 175,\n",
       " 'hat': 176,\n",
       " 'hanging': 177,\n",
       " 'pined': 178,\n",
       " 'cheek': 179,\n",
       " 'beside': 180,\n",
       " 'threaden': 181,\n",
       " 'fillet': 182,\n",
       " 'still': 183,\n",
       " 'bide': 184,\n",
       " 'true': 185,\n",
       " 'bondage': 186,\n",
       " 'would': 187,\n",
       " 'break': 188,\n",
       " 'thence': 189,\n",
       " 'though': 190,\n",
       " 'slackly': 191,\n",
       " 'braided': 192,\n",
       " 'negligence': 193,\n",
       " 'thousand': 194,\n",
       " 'favours': 195,\n",
       " 'maund': 196,\n",
       " 'drew': 197,\n",
       " 'amber': 198,\n",
       " 'crystal': 199,\n",
       " 'beaded': 200,\n",
       " 'jet': 201,\n",
       " 'one': 202,\n",
       " 'by': 203,\n",
       " 'river': 204,\n",
       " 'threw': 205,\n",
       " 'weeping': 206,\n",
       " 'margent': 207,\n",
       " 'was': 208,\n",
       " 'set': 209,\n",
       " 'like': 210,\n",
       " 'usury': 211,\n",
       " 'applying': 212,\n",
       " 'wet': 213,\n",
       " 'or': 214,\n",
       " 'monarch': 215,\n",
       " 'hands': 216,\n",
       " 'let': 217,\n",
       " 'bounty': 218,\n",
       " 'fall': 219,\n",
       " 'where': 220,\n",
       " 'want': 221,\n",
       " 'cries': 222,\n",
       " 'excess': 223,\n",
       " 'begs': 224,\n",
       " 'folded': 225,\n",
       " 'schedules': 226,\n",
       " 'many': 227,\n",
       " 'perused': 228,\n",
       " 'sigh': 229,\n",
       " 'tore': 230,\n",
       " 'gave': 231,\n",
       " 'flood': 232,\n",
       " 'crack': 233,\n",
       " 'ring': 234,\n",
       " 'posied': 235,\n",
       " 'gold': 236,\n",
       " 'bone': 237,\n",
       " 'bidding': 238,\n",
       " 'them': 239,\n",
       " 'find': 240,\n",
       " 'sepulchres': 241,\n",
       " 'mud': 242,\n",
       " 'found': 243,\n",
       " 'yet': 244,\n",
       " 'moe': 245,\n",
       " 'letters': 246,\n",
       " 'sadly': 247,\n",
       " 'penn': 248,\n",
       " 'blood': 249,\n",
       " 'sleided': 250,\n",
       " 'silk': 251,\n",
       " 'feat': 252,\n",
       " 'affectedly': 253,\n",
       " 'enswathed': 254,\n",
       " 'seal': 255,\n",
       " 'curious': 256,\n",
       " 'secrecy': 257,\n",
       " 'these': 258,\n",
       " 'bathed': 259,\n",
       " 'fluxive': 260,\n",
       " 'kiss': 261,\n",
       " \"'gan\": 262,\n",
       " 'tear': 263,\n",
       " 'cried': 264,\n",
       " \"'\": 265,\n",
       " 'o': 266,\n",
       " 'false': 267,\n",
       " 'thou': 268,\n",
       " 'register': 269,\n",
       " 'lies': 270,\n",
       " 'unapproved': 271,\n",
       " 'witness': 272,\n",
       " 'dost': 273,\n",
       " 'bear': 274,\n",
       " '!': 275,\n",
       " 'ink': 276,\n",
       " 'have': 277,\n",
       " 'seem': 278,\n",
       " 'more': 279,\n",
       " 'black': 280,\n",
       " 'damned': 281,\n",
       " 'here': 282,\n",
       " 'said': 283,\n",
       " 'top': 284,\n",
       " 'lines': 285,\n",
       " 'rents': 286,\n",
       " 'big': 287,\n",
       " 'discontent': 288,\n",
       " 'so': 289,\n",
       " 'reverend': 290,\n",
       " 'man': 291,\n",
       " 'grazed': 292,\n",
       " 'his': 293,\n",
       " 'cattle': 294,\n",
       " 'nigh': 295,\n",
       " '--': 296,\n",
       " 'blusterer': 297,\n",
       " 'ruffle': 298,\n",
       " 'knew': 299,\n",
       " 'court': 300,\n",
       " 'city': 301,\n",
       " 'go': 302,\n",
       " 'swiftest': 303,\n",
       " 'hours': 304,\n",
       " 'observed': 305,\n",
       " 'flew': 306,\n",
       " 'towards': 307,\n",
       " 'afflicted': 308,\n",
       " 'fancy': 309,\n",
       " 'fastly': 310,\n",
       " 'privileged': 311,\n",
       " 'desires': 312,\n",
       " 'know': 313,\n",
       " 'brief': 314,\n",
       " 'grounds': 315,\n",
       " 'motives': 316,\n",
       " 'slides': 317,\n",
       " 'he': 318,\n",
       " 'grained': 319,\n",
       " 'bat': 320,\n",
       " 'comely-distant': 321,\n",
       " 'sits': 322,\n",
       " 'side': 323,\n",
       " 'when': 324,\n",
       " 'again': 325,\n",
       " 'being': 326,\n",
       " 'sat': 327,\n",
       " 'grievance': 328,\n",
       " 'hearing': 329,\n",
       " 'divide': 330,\n",
       " 'if': 331,\n",
       " 'him': 332,\n",
       " 'there': 333,\n",
       " 'may': 334,\n",
       " 'be': 335,\n",
       " 'aught': 336,\n",
       " 'applied': 337,\n",
       " 'suffering': 338,\n",
       " 'ecstasy': 339,\n",
       " 'assuage': 340,\n",
       " \"'t\": 341,\n",
       " 'is': 342,\n",
       " 'promised': 343,\n",
       " 'charity': 344,\n",
       " \"'father\": 345,\n",
       " 'says': 346,\n",
       " \"'though\": 347,\n",
       " 'me': 348,\n",
       " 'you': 349,\n",
       " 'behold': 350,\n",
       " 'injury': 351,\n",
       " 'blasting': 352,\n",
       " 'hour': 353,\n",
       " 'tell': 354,\n",
       " 'your': 355,\n",
       " 'judgment': 356,\n",
       " 'am': 357,\n",
       " 'old': 358,\n",
       " 'over': 359,\n",
       " 'hath': 360,\n",
       " 'power': 361,\n",
       " 'been': 362,\n",
       " 'spreading': 363,\n",
       " 'flower': 364,\n",
       " 'fresh': 365,\n",
       " 'myself': 366,\n",
       " 'self-applied': 367,\n",
       " 'love': 368,\n",
       " 'no': 369,\n",
       " \"'but\": 370,\n",
       " 'too': 371,\n",
       " 'early': 372,\n",
       " 'attended': 373,\n",
       " 'youthful': 374,\n",
       " 'suit': 375,\n",
       " 'gain': 376,\n",
       " 'grace': 377,\n",
       " 'nature': 378,\n",
       " 'outwards': 379,\n",
       " 'commended': 380,\n",
       " 'maidens': 381,\n",
       " 'stuck': 382,\n",
       " 'face': 383,\n",
       " 'lack': 384,\n",
       " 'dwelling': 385,\n",
       " 'made': 386,\n",
       " 'fair': 387,\n",
       " 'parts': 388,\n",
       " 'abide': 389,\n",
       " 'new': 390,\n",
       " 'lodged': 391,\n",
       " 'newly': 392,\n",
       " 'deified': 393,\n",
       " \"'his\": 394,\n",
       " 'browny': 395,\n",
       " 'locks': 396,\n",
       " 'hang': 397,\n",
       " 'crooked': 398,\n",
       " 'curls': 399,\n",
       " 'light': 400,\n",
       " 'occasion': 401,\n",
       " 'lips': 402,\n",
       " 'parcels': 403,\n",
       " 'hurls': 404,\n",
       " 'sweet': 405,\n",
       " 'will': 406,\n",
       " 'aptly': 407,\n",
       " 'each': 408,\n",
       " 'eye': 409,\n",
       " 'enchant': 410,\n",
       " 'little': 411,\n",
       " 'drawn': 412,\n",
       " 'largeness': 413,\n",
       " 'thinks': 414,\n",
       " 'paradise': 415,\n",
       " 'sawn': 416,\n",
       " \"'small\": 417,\n",
       " 'show': 418,\n",
       " 'chin': 419,\n",
       " 'phoenix': 420,\n",
       " 'began': 421,\n",
       " 'appear': 422,\n",
       " 'unshorn': 423,\n",
       " 'velvet': 424,\n",
       " 'termless': 425,\n",
       " 'skin': 426,\n",
       " 'bare': 427,\n",
       " 'out-bragg': 428,\n",
       " 'web': 429,\n",
       " 'wear': 430,\n",
       " 'cost': 431,\n",
       " 'dear': 432,\n",
       " 'nice': 433,\n",
       " 'affections': 434,\n",
       " 'wavering': 435,\n",
       " 'stood': 436,\n",
       " 'doubt': 437,\n",
       " 'best': 438,\n",
       " 'were': 439,\n",
       " 'without': 440,\n",
       " 'qualities': 441,\n",
       " 'beauteous': 442,\n",
       " 'form': 443,\n",
       " 'maiden-tongued': 444,\n",
       " 'thereof': 445,\n",
       " 'free': 446,\n",
       " 'men': 447,\n",
       " 'moved': 448,\n",
       " 'such': 449,\n",
       " 'storm': 450,\n",
       " \"'twixt\": 451,\n",
       " 'april': 452,\n",
       " 'see': 453,\n",
       " 'winds': 454,\n",
       " 'breathe': 455,\n",
       " 'untidy': 456,\n",
       " 'rudeness': 457,\n",
       " 'authorized': 458,\n",
       " 'livery': 459,\n",
       " 'falseness': 460,\n",
       " 'truth': 461,\n",
       " \"'well\": 462,\n",
       " 'could': 463,\n",
       " 'say': 464,\n",
       " \"'that\": 465,\n",
       " 'horse': 466,\n",
       " 'mettle': 467,\n",
       " 'rider': 468,\n",
       " 'takes': 469,\n",
       " 'proud': 470,\n",
       " 'subjection': 471,\n",
       " 'noble': 472,\n",
       " 'sway': 473,\n",
       " 'rounds': 474,\n",
       " 'bounds': 475,\n",
       " 'course': 476,\n",
       " 'stop': 477,\n",
       " 'makes': 478,\n",
       " 'controversy': 479,\n",
       " 'hence': 480,\n",
       " 'question': 481,\n",
       " 'whether': 482,\n",
       " 'became': 483,\n",
       " 'deed': 484,\n",
       " 'manage': 485,\n",
       " 'well-doing': 486,\n",
       " 'steed': 487,\n",
       " 'quickly': 488,\n",
       " 'verdict': 489,\n",
       " 'went': 490,\n",
       " 'real': 491,\n",
       " 'habitude': 492,\n",
       " 'life': 493,\n",
       " 'appertainings': 494,\n",
       " 'ornament': 495,\n",
       " 'accomplish': 496,\n",
       " 'himself': 497,\n",
       " 'case': 498,\n",
       " 'aids': 499,\n",
       " 'themselves': 500,\n",
       " 'fairer': 501,\n",
       " 'came': 502,\n",
       " 'additions': 503,\n",
       " 'purposed': 504,\n",
       " 'trim': 505,\n",
       " 'pieced': 506,\n",
       " 'graced': 507,\n",
       " \"'so\": 508,\n",
       " 'tip': 509,\n",
       " 'subduing': 510,\n",
       " 'tongue': 511,\n",
       " 'kinds': 512,\n",
       " 'arguments': 513,\n",
       " 'deep': 514,\n",
       " 'replication': 515,\n",
       " 'prompt': 516,\n",
       " 'reason': 517,\n",
       " 'strong': 518,\n",
       " 'advantage': 519,\n",
       " 'wake': 520,\n",
       " 'sleep': 521,\n",
       " 'make': 522,\n",
       " 'weeper': 523,\n",
       " 'laugh': 524,\n",
       " 'laugher': 525,\n",
       " 'weep': 526,\n",
       " 'dialect': 527,\n",
       " 'different': 528,\n",
       " 'skill': 529,\n",
       " 'catching': 530,\n",
       " 'passions': 531,\n",
       " 'craft': 532,\n",
       " 'general': 533,\n",
       " 'bosom': 534,\n",
       " 'reign': 535,\n",
       " 'young': 536,\n",
       " 'sexes': 537,\n",
       " 'enchanted': 538,\n",
       " 'dwell': 539,\n",
       " 'thoughts': 540,\n",
       " 'remain': 541,\n",
       " 'personal': 542,\n",
       " 'duty': 543,\n",
       " 'following': 544,\n",
       " 'haunted': 545,\n",
       " 'consents': 546,\n",
       " 'bewitch': 547,\n",
       " 'desire': 548,\n",
       " 'granted': 549,\n",
       " 'dialogued': 550,\n",
       " 'ask': 551,\n",
       " 'own': 552,\n",
       " 'wills': 553,\n",
       " 'obey': 554,\n",
       " \"'many\": 555,\n",
       " 'picture': 556,\n",
       " 'get': 557,\n",
       " 'serve': 558,\n",
       " 'put': 559,\n",
       " 'fools': 560,\n",
       " 'th': 561,\n",
       " 'imagination': 562,\n",
       " 'goodly': 563,\n",
       " 'objects': 564,\n",
       " 'abroad': 565,\n",
       " 'lands': 566,\n",
       " 'mansions': 567,\n",
       " 'theirs': 568,\n",
       " 'assign': 569,\n",
       " 'labouring': 570,\n",
       " 'pleasures': 571,\n",
       " 'bestow': 572,\n",
       " 'than': 573,\n",
       " 'gouty': 574,\n",
       " 'landlord': 575,\n",
       " 'doth': 576,\n",
       " 'owe': 577,\n",
       " 'never': 578,\n",
       " 'touch': 579,\n",
       " 'sweetly': 580,\n",
       " 'supposed': 581,\n",
       " 'mistress': 582,\n",
       " 'heart': 583,\n",
       " 'woeful': 584,\n",
       " 'self': 585,\n",
       " 'freedom': 586,\n",
       " 'stand': 587,\n",
       " 'fee-simple': 588,\n",
       " 'part': 589,\n",
       " 'art': 590,\n",
       " 'charmed': 591,\n",
       " 'reserved': 592,\n",
       " 'stalk': 593,\n",
       " \"'yet\": 594,\n",
       " 'equals': 595,\n",
       " 'demand': 596,\n",
       " 'desired': 597,\n",
       " 'yielded': 598,\n",
       " 'finding': 599,\n",
       " 'honour': 600,\n",
       " 'forbid': 601,\n",
       " 'safest': 602,\n",
       " 'distance': 603,\n",
       " 'mine': 604,\n",
       " 'shielded': 605,\n",
       " 'experience': 606,\n",
       " 'bulwarks': 607,\n",
       " 'builded': 608,\n",
       " 'proofs': 609,\n",
       " 'new-bleeding': 610,\n",
       " 'foil': 611,\n",
       " 'jewel': 612,\n",
       " 'amorous': 613,\n",
       " 'spoil': 614,\n",
       " 'ah': 615,\n",
       " 'who': 616,\n",
       " 'ever': 617,\n",
       " 'shunn': 618,\n",
       " 'precedent': 619,\n",
       " 'destined': 620,\n",
       " 'ill': 621,\n",
       " 'must': 622,\n",
       " 'herself': 623,\n",
       " 'assay': 624,\n",
       " '?': 625,\n",
       " 'forced': 626,\n",
       " 'examples': 627,\n",
       " \"'gainst\": 628,\n",
       " 'content': 629,\n",
       " 'by-past': 630,\n",
       " 'perils': 631,\n",
       " 'way': 632,\n",
       " 'counsel': 633,\n",
       " 'awhile': 634,\n",
       " 'stay': 635,\n",
       " 'we': 636,\n",
       " 'advice': 637,\n",
       " 'seen': 638,\n",
       " 'blunting': 639,\n",
       " 'us': 640,\n",
       " 'our': 641,\n",
       " 'wits': 642,\n",
       " 'keen': 643,\n",
       " \"'nor\": 644,\n",
       " 'gives': 645,\n",
       " 'satisfaction': 646,\n",
       " 'curb': 647,\n",
       " 'others': 648,\n",
       " 'proof': 649,\n",
       " 'forbod': 650,\n",
       " 'sweets': 651,\n",
       " 'good': 652,\n",
       " 'fear': 653,\n",
       " 'harms': 654,\n",
       " 'preach': 655,\n",
       " 'behoof': 656,\n",
       " 'appetite': 657,\n",
       " 'aloof': 658,\n",
       " 'palate': 659,\n",
       " 'needs': 660,\n",
       " 'taste': 661,\n",
       " 'cry': 662,\n",
       " \"'it\": 663,\n",
       " 'thy': 664,\n",
       " 'last': 665,\n",
       " \"'for\": 666,\n",
       " 'further': 667,\n",
       " \"'this\": 668,\n",
       " 'untrue': 669,\n",
       " 'patterns': 670,\n",
       " 'foul': 671,\n",
       " 'beguiling': 672,\n",
       " 'heard': 673,\n",
       " 'plants': 674,\n",
       " 'orchards': 675,\n",
       " 'grew': 676,\n",
       " 'how': 677,\n",
       " 'deceits': 678,\n",
       " 'gilded': 679,\n",
       " 'smiling': 680,\n",
       " 'vows': 681,\n",
       " 'brokers': 682,\n",
       " 'defiling': 683,\n",
       " 'words': 684,\n",
       " 'merely': 685,\n",
       " 'bastards': 686,\n",
       " 'adulterate': 687,\n",
       " \"'and\": 688,\n",
       " 'terms': 689,\n",
       " 'held': 690,\n",
       " 'till': 691,\n",
       " 'thus': 692,\n",
       " 'gan': 693,\n",
       " 'besiege': 694,\n",
       " \"'gentle\": 695,\n",
       " 'feeling': 696,\n",
       " 'pity': 697,\n",
       " 'holy': 698,\n",
       " 'afraid': 699,\n",
       " 'ye': 700,\n",
       " 'sworn': 701,\n",
       " 'none': 702,\n",
       " 'feasts': 703,\n",
       " 'call': 704,\n",
       " 'unto': 705,\n",
       " 'now': 706,\n",
       " \"ne'er\": 707,\n",
       " 'invite': 708,\n",
       " 'woo': 709,\n",
       " \"''\": 710,\n",
       " 'offences': 711,\n",
       " 'errors': 712,\n",
       " 'acture': 713,\n",
       " 'neither': 714,\n",
       " 'party': 715,\n",
       " 'kind': 716,\n",
       " 'sought': 717,\n",
       " 'shame': 718,\n",
       " 'much': 719,\n",
       " 'less': 720,\n",
       " 'remains': 721,\n",
       " 'reproach': 722,\n",
       " 'contains': 723,\n",
       " 'among': 724,\n",
       " 'flame': 725,\n",
       " 'warm': 726,\n",
       " 'affection': 727,\n",
       " 'smallest': 728,\n",
       " 'teen': 729,\n",
       " 'any': 730,\n",
       " 'leisures': 731,\n",
       " 'charm': 732,\n",
       " 'harm': 733,\n",
       " 'kept': 734,\n",
       " 'hearts': 735,\n",
       " 'liveries': 736,\n",
       " 'commanding': 737,\n",
       " 'monarchy': 738,\n",
       " 'look': 739,\n",
       " 'tributes': 740,\n",
       " 'wounded': 741,\n",
       " 'fancies': 742,\n",
       " 'sent': 743,\n",
       " 'paled': 744,\n",
       " 'pearls': 745,\n",
       " 'rubies': 746,\n",
       " 'red': 747,\n",
       " 'figuring': 748,\n",
       " 'likewise': 749,\n",
       " 'lent': 750,\n",
       " 'grief': 751,\n",
       " 'blushes': 752,\n",
       " 'understood': 753,\n",
       " 'bloodless': 754,\n",
       " 'white': 755,\n",
       " 'encrimson': 756,\n",
       " 'mood': 757,\n",
       " 'effects': 758,\n",
       " 'terror': 759,\n",
       " 'modesty': 760,\n",
       " 'encamp': 761,\n",
       " 'fighting': 762,\n",
       " 'outwardly': 763,\n",
       " 'lo': 764,\n",
       " 'talents': 765,\n",
       " 'twisted': 766,\n",
       " 'metal': 767,\n",
       " 'amorously': 768,\n",
       " 'impleach': 769,\n",
       " 'received': 770,\n",
       " 'several': 771,\n",
       " 'acceptance': 772,\n",
       " 'weepingly': 773,\n",
       " 'beseech': 774,\n",
       " 'annexions': 775,\n",
       " 'gems': 776,\n",
       " 'enrich': 777,\n",
       " 'deep-brain': 778,\n",
       " 'sonnets': 779,\n",
       " 'amplify': 780,\n",
       " 'stone': 781,\n",
       " 'worth': 782,\n",
       " 'quality': 783,\n",
       " 'diamond': 784,\n",
       " 'why': 785,\n",
       " 'beautiful': 786,\n",
       " 'hard': 787,\n",
       " 'whereto': 788,\n",
       " 'invised': 789,\n",
       " 'properties': 790,\n",
       " 'tend': 791,\n",
       " 'deep-green': 792,\n",
       " 'emerald': 793,\n",
       " 'regard': 794,\n",
       " 'weak': 795,\n",
       " 'sights': 796,\n",
       " 'sickly': 797,\n",
       " 'radiance': 798,\n",
       " 'amend': 799,\n",
       " 'heaven-hued': 800,\n",
       " 'sapphire': 801,\n",
       " 'opal': 802,\n",
       " 'blend': 803,\n",
       " 'manifold': 804,\n",
       " 'wit': 805,\n",
       " 'well': 806,\n",
       " 'blazon': 807,\n",
       " 'smiled': 808,\n",
       " 'moan': 809,\n",
       " 'trophies': 810,\n",
       " 'hot': 811,\n",
       " 'pensived': 812,\n",
       " 'subdued': 813,\n",
       " 'tender': 814,\n",
       " 'charged': 815,\n",
       " 'hoard': 816,\n",
       " 'yield': 817,\n",
       " 'up': 818,\n",
       " 'render': 819,\n",
       " 'origin': 820,\n",
       " 'ender': 821,\n",
       " 'force': 822,\n",
       " 'oblations': 823,\n",
       " 'since': 824,\n",
       " 'altar': 825,\n",
       " 'enpatron': 826,\n",
       " 'then': 827,\n",
       " 'advance': 828,\n",
       " 'yours': 829,\n",
       " 'phraseless': 830,\n",
       " 'weighs': 831,\n",
       " 'airy': 832,\n",
       " 'scale': 833,\n",
       " 'praise': 834,\n",
       " 'take': 835,\n",
       " 'similes': 836,\n",
       " 'command': 837,\n",
       " 'hallow': 838,\n",
       " 'sighs': 839,\n",
       " 'burning': 840,\n",
       " 'lungs': 841,\n",
       " 'raise': 842,\n",
       " 'minister': 843,\n",
       " 'obeys': 844,\n",
       " 'works': 845,\n",
       " 'under': 846,\n",
       " 'audit': 847,\n",
       " 'comes': 848,\n",
       " 'distract': 849,\n",
       " 'combined': 850,\n",
       " 'sums': 851,\n",
       " 'device': 852,\n",
       " 'nun': 853,\n",
       " 'sister': 854,\n",
       " 'sanctified': 855,\n",
       " 'holiest': 856,\n",
       " 'note': 857,\n",
       " 'late': 858,\n",
       " 'shun': 859,\n",
       " 'rarest': 860,\n",
       " 'havings': 861,\n",
       " 'blossoms': 862,\n",
       " 'dote': 863,\n",
       " 'richest': 864,\n",
       " 'coat': 865,\n",
       " 'cold': 866,\n",
       " 'remove': 867,\n",
       " 'spend': 868,\n",
       " 'living': 869,\n",
       " 'eternal': 870,\n",
       " 'labour': 871,\n",
       " \"is't\": 872,\n",
       " 'leave': 873,\n",
       " 'thing': 874,\n",
       " 'mastering': 875,\n",
       " 'strives': 876,\n",
       " 'playing': 877,\n",
       " 'receive': 878,\n",
       " 'patient': 879,\n",
       " 'sports': 880,\n",
       " 'unconstrained': 881,\n",
       " 'gyves': 882,\n",
       " 'fame': 883,\n",
       " 'contrives': 884,\n",
       " 'scars': 885,\n",
       " 'battle': 886,\n",
       " \"'scapeth\": 887,\n",
       " 'flight': 888,\n",
       " 'absence': 889,\n",
       " 'valiant': 890,\n",
       " 'pardon': 891,\n",
       " 'boast': 892,\n",
       " 'accident': 893,\n",
       " 'brought': 894,\n",
       " 'moment': 895,\n",
       " 'subdue': 896,\n",
       " 'caged': 897,\n",
       " 'cloister': 898,\n",
       " 'fly': 899,\n",
       " 'religious': 900,\n",
       " 'out': 901,\n",
       " 'religion': 902,\n",
       " 'tempted': 903,\n",
       " 'immured': 904,\n",
       " 'tempt': 905,\n",
       " 'liberty': 906,\n",
       " 'procured': 907,\n",
       " 'mighty': 908,\n",
       " 'hear': 909,\n",
       " 'broken': 910,\n",
       " 'bosoms': 911,\n",
       " 'belong': 912,\n",
       " 'emptied': 913,\n",
       " 'fountains': 914,\n",
       " 'pour': 915,\n",
       " 'ocean': 916,\n",
       " \"o'er\": 917,\n",
       " 'victory': 918,\n",
       " 'congest': 919,\n",
       " 'compound': 920,\n",
       " 'physic': 921,\n",
       " 'breast': 922,\n",
       " 'sacred': 923,\n",
       " 'disciplined': 924,\n",
       " 'ay': 925,\n",
       " 'dieted': 926,\n",
       " 'believed': 927,\n",
       " 'assail': 928,\n",
       " 'consecrations': 929,\n",
       " 'giving': 930,\n",
       " 'most': 931,\n",
       " 'potential': 932,\n",
       " 'vow': 933,\n",
       " 'bond': 934,\n",
       " 'space': 935,\n",
       " 'thee': 936,\n",
       " 'sting': 937,\n",
       " 'knot': 938,\n",
       " 'confine': 939,\n",
       " 'things': 940,\n",
       " 'else': 941,\n",
       " 'thine': 942,\n",
       " 'impressest': 943,\n",
       " 'precepts': 944,\n",
       " 'stale': 945,\n",
       " 'example': 946,\n",
       " 'wilt': 947,\n",
       " 'inflame': 948,\n",
       " 'coldly': 949,\n",
       " 'those': 950,\n",
       " 'impediments': 951,\n",
       " 'forth': 952,\n",
       " 'wealth': 953,\n",
       " 'filial': 954,\n",
       " 'law': 955,\n",
       " 'kindred': 956,\n",
       " 'arms': 957,\n",
       " 'peace': 958,\n",
       " 'rule': 959,\n",
       " 'sense': 960,\n",
       " 'sweetens': 961,\n",
       " 'pangs': 962,\n",
       " 'aloes': 963,\n",
       " 'forces': 964,\n",
       " 'shocks': 965,\n",
       " 'fears': 966,\n",
       " 'depend': 967,\n",
       " 'bleeding': 968,\n",
       " 'groans': 969,\n",
       " 'pine': 970,\n",
       " 'supplicant': 971,\n",
       " 'lending': 972,\n",
       " 'soft': 973,\n",
       " 'audience': 974,\n",
       " 'design': 975,\n",
       " 'credent': 976,\n",
       " 'soul': 977,\n",
       " 'strong-bonded': 978,\n",
       " 'oath': 979,\n",
       " 'shall': 980,\n",
       " 'prefer': 981,\n",
       " 'undertake': 982,\n",
       " 'troth': 983,\n",
       " 'watery': 984,\n",
       " 'dismount': 985,\n",
       " 'running': 986,\n",
       " 'fount': 987,\n",
       " 'brinish': 988,\n",
       " 'current': 989,\n",
       " 'downward': 990,\n",
       " 'flow': 991,\n",
       " 'apace': 992,\n",
       " 'channel': 993,\n",
       " 'stream': 994,\n",
       " 'glazed': 995,\n",
       " 'gate': 996,\n",
       " 'glowing': 997,\n",
       " 'roses': 998,\n",
       " 'water': 999,\n",
       " 'hue': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab = create_vocab(lines)\n",
    "Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b734d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27455"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33b351",
   "metadata": {},
   "source": [
    "<a name='ex-1'></a>\n",
    "### line_to_tensor\n",
    "\n",
    "Write a function that takes in a single line and transforms each word into its unicode integer.  This returns a list of integers, which we'll refer to as a tensor.\n",
    "- Use a special integer to represent the end of the sentence (the end of the line).\n",
    "- This will be the EOS_int (end of sentence integer) parameter of the function.\n",
    "- Include the EOS_int as the last integer of the line\n",
    "- We will use the number `1` to represent the end of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57a0d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_tensor(line, vocab, EOS_int=1):\n",
    "    \"\"\"Turns a line of text into a tensor\n",
    "\n",
    "    Args:\n",
    "        line (str): A single line of text.\n",
    "        EOS_int (int, optional): End-of-sentence integer. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        list: a list of integers (unicode values) for the words in the `line`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the tensor as an empty list\n",
    "    tensor = []\n",
    "    \n",
    "    # for each character:\n",
    "    for word in process_tweet(line):\n",
    "        \n",
    "        # convert to unicode int\n",
    "        w_int = vocab[word]\n",
    "        \n",
    "        # append the unicode integer to the tensor list\n",
    "        tensor.append(w_int)\n",
    "    \n",
    "    # include the end-of-sentence integer\n",
    "    tensor.append(EOS_int)\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "74e31b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51, 22, 1434, 4, 2077, 26, 2078, 31, 2079, 1]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_to_tensor(\"With this night's revels and expire the term\", Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9d463",
   "metadata": {},
   "source": [
    "<a name='1-3'></a>\n",
    "### 1.3 - Batch Generator \n",
    "\n",
    "Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets. Here, we will build a data generator that takes in a text and returns a batch of text lines (lines are sentences).\n",
    "- The generator converts text lines (sentences) into numpy arrays of integers padded by zeros so that all arrays have the same length, which is the length of the longest sentence in the entire data set.\n",
    "\n",
    "Once we create the generator, we can iterate on it like this:\n",
    "\n",
    "```\n",
    "next(data_generator)\n",
    "```\n",
    "\n",
    "This generator returns the data in a format that we could directly use in our model when computing the feed-forward of our algorithm. This iterator returns a batch of lines and per token mask. The batch is a tuple of three parts: inputs, targets, mask. The inputs and targets are identical. The second column will be used to evaluate our predictions. Mask is 1 for non-padding tokens.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### data_generator\n",
    "- While True loop: this will yield one batch at a time.\n",
    "- if index >= num_lines, set index to 0. \n",
    "- The generator should return shuffled batches of data. To achieve this without modifying the actual lines a list containing the indexes of `data_lines` is created. This list can be shuffled and used to get random batches everytime the index is reset.\n",
    "- if len(line) < max_length append line to cur_batch.\n",
    "    - Note that a line that has length equal to max_length should not be appended to the batch. \n",
    "    - This is because when converting the characters into a tensor of integers, an additional end of sentence token id will be added.  \n",
    "    - So if max_length is 5, and a line has 4 characters, the tensor representing those 4 characters plus the end of sentence character will be of length 5, which is the max length.\n",
    "- if len(cur_batch) == batch_size, go over every line, convert it to an int and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6eb8625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, max_length, data_lines, vocab, line_to_tensor=line_to_tensor, shuffle=True):\n",
    "    \"\"\"Generator function that yields batches of data\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): number of examples (in this case, sentences) per batch.\n",
    "        max_length (int): maximum length of the output tensor.\n",
    "        NOTE: max_length includes the end-of-sentence character that will be added\n",
    "                to the tensor.  \n",
    "                Keep in mind that the length of the tensor is always 1 + the length\n",
    "                of the original line of characters.\n",
    "        data_lines (list): list of the sentences to group into batches.\n",
    "        line_to_tensor (function, optional): function that converts line to tensor. Defaults to line_to_tensor.\n",
    "        shuffle (bool, optional): True if the generator should generate random batches of data. Defaults to True.\n",
    "\n",
    "    Yields:\n",
    "        tuple: two copies of the batch (jax.interpreters.xla.DeviceArray) and mask (jax.interpreters.xla.DeviceArray).\n",
    "    \"\"\"\n",
    "    # initialize the index that points to the current position in the lines index array\n",
    "    index = 0\n",
    "    \n",
    "    # initialize the list that will contain the current batch\n",
    "    cur_batch = []\n",
    "    \n",
    "    # count the number of lines in data_lines\n",
    "    num_lines = len(data_lines)\n",
    "    \n",
    "    # create an array with the indexes of data_lines that can be shuffled\n",
    "    lines_index = [*range(num_lines)]\n",
    "    \n",
    "    # shuffle line indexes if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(lines_index)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # if the index is greater than or equal to the number of lines in data_lines\n",
    "        if index >= num_lines:\n",
    "            # then reset the index to 0\n",
    "            index = 0\n",
    "            # shuffle line indexes if shuffle is set to True\n",
    "            if shuffle:\n",
    "                rnd.shuffle(lines_index) \n",
    "                            \n",
    "        # get a line at the `lines_index[index]` position in data_lines\n",
    "        line = data_lines[lines_index[index]]\n",
    "        \n",
    "        # if the length of the line is less than max_length\n",
    "        if len(line_to_tensor(line, vocab)) < max_length:\n",
    "            # append the line to the current batch\n",
    "            cur_batch.append(line)\n",
    "            \n",
    "        # increment the index by one\n",
    "        index += 1\n",
    "        \n",
    "        # if the current batch is now equal to the desired batch size\n",
    "        if len(cur_batch) == batch_size:\n",
    "            \n",
    "            batch = []\n",
    "            mask = []\n",
    "            \n",
    "            # go through each line (li) in cur_batch\n",
    "            for li in cur_batch:\n",
    "                # convert the line (li) to a tensor of integers\n",
    "                tensor = line_to_tensor(li, vocab)\n",
    "                \n",
    "                # Create a list of zeros to represent the padding\n",
    "                # so that the tensor plus padding will have length `max_length`\n",
    "                pad = [0] * (max_length - len(tensor))\n",
    "                \n",
    "                # combine the tensor plus pad\n",
    "                tensor_pad = tensor + pad\n",
    "                \n",
    "                # append the padded tensor to the batch\n",
    "                batch.append(tensor_pad)\n",
    "\n",
    "                # A mask for this tensor_pad is 1 whereever tensor_pad is not\n",
    "                # 0 and 0 whereever tensor_pad is 0, i.e. if tensor_pad is\n",
    "                # [1, 2, 3, 0, 0, 0] then example_mask should be\n",
    "                # [1, 1, 1, 0, 0, 0]\n",
    "                example_mask = 1 - np.equal(np.array(tensor_pad), 0)\n",
    "                mask.append(example_mask) # @ KEEPTHIS\n",
    "               \n",
    "            # convert the batch (data type list) to a numpy array\n",
    "            batch_np_arr = np.array(batch)\n",
    "            mask_np_arr = np.array(mask)\n",
    "            \n",
    "            # Yield two copies of the batch and mask.\n",
    "            yield batch_np_arr, batch_np_arr, mask_np_arr\n",
    "            \n",
    "            # reset the current batch to an empty list\n",
    "            cur_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f91b562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4275,  342, 5438,    1,    0,    0,    0,    0,    0,    0],\n",
       "        [ 998,  143,  747,    1,    0,    0,    0,    0,    0,    0]]),\n",
       " array([[4275,  342, 5438,    1,    0,    0,    0,    0,    0,    0],\n",
       "        [ 998,  143,  747,    1,    0,    0,    0,    0,    0,    0]]),\n",
       " array([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out our data generator\n",
    "tmp_lines = ['Sky is blue', #length 11\n",
    "             'Roses are red', # length 9\n",
    "             'Leaves are green', # length 9\n",
    "             'I love you'] # length 9\n",
    "\n",
    "# Get a batch size of 2, max length 10\n",
    "tmp_data_gen = data_generator(batch_size=2, \n",
    "                              max_length=10, \n",
    "                              data_lines=tmp_lines,\n",
    "                              vocab = Vocab,\n",
    "                              shuffle=False)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_data_gen)\n",
    "\n",
    "# view the batch\n",
    "tmp_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b214938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
